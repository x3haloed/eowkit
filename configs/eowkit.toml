[model]
# Ollama model name or empty if using mobile MLC only
ollama = "llama3.1:8b-instruct-fp16"

[mobile]
# For phone builds (MLC). Optional here; the installer will present choices.
mlc_model = "mlc-ai/Llama-3.2-1B-Instruct-q0f16-MLC"

[paths]
# Where large downloads are staged (archives/ZIMs). Used by the installer.
downloads_dir = "downloads"
# Where .zim files live (final location). The installer copies here from downloads_dir.
zim_dir       = "downloads"
# Where Ollama models and local tools live.
models_dir    = "models"
# Local kiwix-tools placement (if downloaded); kiwix-serve will be copied here.
kiwix_tools_dir = "models/tools"
# Set by installer if a local kiwix-serve was placed (absolute path to binary). Leave empty to auto-detect.
kiwix_serve_bin = ""
# Local Ollama placement (if downloaded). The binary and lib/ will be copied here.
ollama_dir = "models/ollama"
# Set by installer if a local ollama binary was placed (absolute path to binary). Leave empty to auto-detect.
ollama_bin = ""

[wiki]
zim = "/data/wikipedia_en_all_nopic_2025-08.zim"
kiwix_port = 8080
bind = "127.0.0.1"

[llm]
ollama_url = "http://127.0.0.1:11434"
context_tokens = 4096
temperature = 0.2

[retrieval]
k = 40
max_articles = 5
rerank = false

[reranker]
enabled = false
onnx_model = "models/cross-encoder-msmarco-MiniLM-L-6-v2.onnx"   # ~80-100MB
tokenizer_vocab = "models/vocab.txt"                             # BERT WordPiece
max_seq_len = 256

[prompt]
system = """
You are an offline encyclopedia assistant. Always call tools to search the local Wikipedia snapshot before answering.
Cite article titles. If nothing is found, say 'No support found in this offline snapshot.'
"""